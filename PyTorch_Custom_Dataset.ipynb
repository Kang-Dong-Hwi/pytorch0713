{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spoken-digit-dataset\n",
    "#### https://www.kaggle.com/divyanshu99/spoken-digit-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 50\n",
    "PATH = \"C:/Projects/keras_talk/keras/intern/0713/recordings/\"\n",
    "file_list = os.listdir(PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### if  (x_data size < N)  -->  zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def padding(x_data, N):\n",
    "    if len(x_data) >= N:\n",
    "        return x_data[:N]\n",
    "    else:\n",
    "        zeros = np.asarray([0]*(N-len(x_data)))\n",
    "        return np.append(np.transpose(x_data)[0], zeros) if len(x_data.shape) == 2 else np.append(x_data, zeros)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### size of test_dataset : 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#test_set_index        \n",
    "test_idx = []\n",
    "num = random.randrange(0,1500)\n",
    "for i in range(100):\n",
    "    while num in test_idx:\n",
    "        num = random.randrange(0,1500)\n",
    "    test_idx.append(num)\n",
    "test_idx = np.asarray(test_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### wav -> numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DongHwi\\anaconda3\\envs\\venv\\lib\\site-packages\\ipykernel_launcher.py:8: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test  = []\n",
    "y_test  = []\n",
    "\n",
    "\n",
    "for i,file in enumerate(file_list):\n",
    "    digit_sound = padding( wavfile.read(PATH+file)[1] , 5000)\n",
    "    #digit_sound = noise(   wavfile.read(PATH+file)[1] , 5000)\n",
    "    digit = int( file[0] )\n",
    "    if len(digit_sound.shape) == 2:\n",
    "        digit_sound = np.transpose(digit_sound)[0]\n",
    "\n",
    "    if i in test_idx:\n",
    "        x_test.append(digit_sound)\n",
    "        y_test.append(digit)\n",
    "    else:\n",
    "        x_train.append(digit_sound)\n",
    "        y_train.append(digit)\n",
    "    \n",
    "    \n",
    "x_train = np.asarray(x_train, dtype = np.float32).reshape(1400,1,5000)\n",
    "y_train = np.asarray(y_train)\n",
    "x_test  = np.asarray(x_test, dtype = np.float32).reshape(100,1,5000)\n",
    "y_test  = np.asarray(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test  = torch.from_numpy(x_test ).float().to('cpu')\n",
    "x_train = torch.from_numpy(x_train).float().to('cpu')\n",
    "y_test  = torch.from_numpy(y_test ).long().to('cpu')\n",
    "y_train = torch.from_numpy(y_train).long().to('cpu')\n",
    "\n",
    "\n",
    "dataset_train2 = TensorDataset(x_train, y_train)\n",
    "dataset_test2  = TensorDataset(x_test , y_test )\n",
    "\n",
    "train_dataset = DataLoader(dataset = dataset_train2, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)\n",
    "test_dataset  = DataLoader(dataset = dataset_test2 , batch_size = BATCH_SIZE, shuffle = True, drop_last = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "\n",
    "class CNN_spoken_digit_dataset( nn.Module ):\n",
    "    def __init__(self):\n",
    "        super(CNN_spoken_digit_dataset, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d( in_channels = 1,   out_channels = 128, kernel_size = 2 )\n",
    "        self.conv2 = nn.Conv1d( in_channels = 128, out_channels = 64,  kernel_size = 2 )\n",
    "        self.conv3 = nn.Conv1d( in_channels = 64,  out_channels = 32,  kernel_size = 2 )\n",
    "        \n",
    "        self.maxp1 = nn.MaxPool1d( kernel_size = 2, stride = 5 )\n",
    "        self.maxp2 = nn.MaxPool1d( kernel_size = 2, stride = 5 )\n",
    "        \n",
    "        self.lay1  = nn.Linear( 6400, 128 )\n",
    "        self.lay2  = nn.Linear( 128 , 64 )\n",
    "        self.lay3  = nn.Linear( 64 , 10 )\n",
    "        \n",
    "        self.relu  = nn.ReLU()\n",
    "        self.drop  = nn.Dropout()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.relu(output)\n",
    "        output = self.maxp1(output)\n",
    "        output = self.drop(output)\n",
    "        \n",
    "        output = self.conv2(output)\n",
    "        output = self.relu(output)\n",
    "        \n",
    "        output = self.conv3(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.maxp2(output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output = output.view(-1, 6400)\n",
    "        output = self.drop(output)\n",
    "        \n",
    "        output = self.lay1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.drop(output)\n",
    "        \n",
    "        output = self.lay2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.drop(output)\n",
    "        \n",
    "        output = self.lay3(output)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n",
      "\tLoss:46.091\tAcc:12.000\n",
      "\tLoss:49.955\tAcc:14.000\n",
      "\tLoss:26.718\tAcc:8.000\n",
      "\tLoss:32.560\tAcc:14.000\n",
      "\tLoss:40.198\tAcc:4.000\n",
      "\tLoss:38.082\tAcc:10.000\n",
      "\tLoss:18.174\tAcc:12.000\n",
      "\tLoss:22.824\tAcc:2.000\n",
      "\tLoss:17.322\tAcc:6.000\n",
      "\tLoss:27.949\tAcc:12.000\n",
      "\tLoss:17.478\tAcc:10.000\n",
      "\tLoss:18.019\tAcc:10.000\n",
      "\tLoss:11.218\tAcc:12.000\n",
      "\tLoss:17.448\tAcc:16.000\n",
      "\tLoss:16.741\tAcc:14.000\n",
      "\tLoss:14.438\tAcc:10.000\n",
      "\tLoss:13.607\tAcc:10.000\n",
      "\tLoss:13.542\tAcc:12.000\n",
      "\tLoss:11.187\tAcc:6.000\n",
      "\tLoss:11.360\tAcc:6.000\n",
      "\tLoss:12.291\tAcc:8.000\n",
      "\tLoss:12.130\tAcc:10.000\n",
      "\tLoss:14.176\tAcc:6.000\n",
      "\tLoss:8.058\tAcc:14.000\n",
      "\tLoss:7.797\tAcc:8.000\n",
      "\tLoss:7.297\tAcc:12.000\n",
      "\tLoss:7.506\tAcc:14.000\n",
      "\tLoss:10.080\tAcc:8.000\n",
      "epoch2\n",
      "\tLoss:8.294\tAcc:0.000\n",
      "\tLoss:6.636\tAcc:6.000\n",
      "\tLoss:7.981\tAcc:12.000\n",
      "\tLoss:4.558\tAcc:8.000\n",
      "\tLoss:5.546\tAcc:16.000\n",
      "\tLoss:6.542\tAcc:4.000\n",
      "\tLoss:8.641\tAcc:4.000\n",
      "\tLoss:5.272\tAcc:4.000\n",
      "\tLoss:6.751\tAcc:10.000\n",
      "\tLoss:6.953\tAcc:10.000\n",
      "\tLoss:5.333\tAcc:12.000\n",
      "\tLoss:5.449\tAcc:10.000\n",
      "\tLoss:5.424\tAcc:4.000\n",
      "\tLoss:3.455\tAcc:20.000\n",
      "\tLoss:4.127\tAcc:12.000\n",
      "\tLoss:3.583\tAcc:10.000\n",
      "\tLoss:3.917\tAcc:10.000\n",
      "\tLoss:3.632\tAcc:2.000\n",
      "\tLoss:5.937\tAcc:10.000\n",
      "\tLoss:4.956\tAcc:12.000\n",
      "\tLoss:4.757\tAcc:4.000\n",
      "\tLoss:4.190\tAcc:6.000\n",
      "\tLoss:3.375\tAcc:6.000\n",
      "\tLoss:6.016\tAcc:14.000\n",
      "\tLoss:5.159\tAcc:6.000\n",
      "\tLoss:4.175\tAcc:14.000\n",
      "\tLoss:3.300\tAcc:12.000\n",
      "\tLoss:4.247\tAcc:16.000\n",
      "epoch3\n",
      "\tLoss:3.857\tAcc:4.000\n",
      "\tLoss:5.365\tAcc:18.000\n",
      "\tLoss:2.729\tAcc:14.000\n",
      "\tLoss:2.899\tAcc:12.000\n",
      "\tLoss:4.395\tAcc:4.000\n",
      "\tLoss:2.742\tAcc:10.000\n",
      "\tLoss:4.015\tAcc:8.000\n",
      "\tLoss:4.621\tAcc:8.000\n",
      "\tLoss:4.118\tAcc:14.000\n",
      "\tLoss:3.229\tAcc:16.000\n",
      "\tLoss:3.363\tAcc:8.000\n",
      "\tLoss:4.817\tAcc:2.000\n",
      "\tLoss:4.023\tAcc:6.000\n",
      "\tLoss:5.955\tAcc:12.000\n",
      "\tLoss:4.497\tAcc:10.000\n",
      "\tLoss:2.701\tAcc:18.000\n",
      "\tLoss:3.717\tAcc:4.000\n",
      "\tLoss:3.332\tAcc:6.000\n",
      "\tLoss:2.869\tAcc:8.000\n",
      "\tLoss:3.394\tAcc:6.000\n",
      "\tLoss:3.405\tAcc:18.000\n",
      "\tLoss:3.356\tAcc:6.000\n",
      "\tLoss:3.402\tAcc:8.000\n",
      "\tLoss:3.643\tAcc:6.000\n",
      "\tLoss:3.905\tAcc:6.000\n",
      "\tLoss:3.817\tAcc:8.000\n",
      "\tLoss:2.298\tAcc:8.000\n",
      "\tLoss:3.712\tAcc:16.000\n",
      "epoch4\n",
      "\tLoss:2.439\tAcc:8.000\n",
      "\tLoss:4.046\tAcc:6.000\n",
      "\tLoss:2.367\tAcc:12.000\n",
      "\tLoss:2.855\tAcc:8.000\n",
      "\tLoss:2.485\tAcc:12.000\n",
      "\tLoss:3.593\tAcc:6.000\n",
      "\tLoss:2.815\tAcc:12.000\n",
      "\tLoss:3.148\tAcc:4.000\n",
      "\tLoss:3.515\tAcc:10.000\n",
      "\tLoss:2.821\tAcc:6.000\n",
      "\tLoss:2.533\tAcc:18.000\n",
      "\tLoss:2.327\tAcc:10.000\n",
      "\tLoss:3.168\tAcc:10.000\n",
      "\tLoss:3.713\tAcc:8.000\n",
      "\tLoss:3.134\tAcc:6.000\n",
      "\tLoss:2.305\tAcc:14.000\n",
      "\tLoss:2.655\tAcc:4.000\n",
      "\tLoss:2.517\tAcc:10.000\n",
      "\tLoss:2.482\tAcc:12.000\n",
      "\tLoss:2.863\tAcc:4.000\n",
      "\tLoss:2.981\tAcc:14.000\n",
      "\tLoss:2.826\tAcc:10.000\n",
      "\tLoss:2.365\tAcc:12.000\n",
      "\tLoss:2.690\tAcc:10.000\n",
      "\tLoss:2.431\tAcc:18.000\n",
      "\tLoss:2.689\tAcc:2.000\n",
      "\tLoss:2.223\tAcc:14.000\n",
      "\tLoss:2.865\tAcc:10.000\n",
      "epoch5\n",
      "\tLoss:2.736\tAcc:8.000\n",
      "\tLoss:3.142\tAcc:4.000\n",
      "\tLoss:3.069\tAcc:16.000\n",
      "\tLoss:3.096\tAcc:14.000\n",
      "\tLoss:3.136\tAcc:14.000\n",
      "\tLoss:2.717\tAcc:14.000\n",
      "\tLoss:2.574\tAcc:12.000\n",
      "\tLoss:2.826\tAcc:16.000\n",
      "\tLoss:2.419\tAcc:10.000\n",
      "\tLoss:2.354\tAcc:4.000\n",
      "\tLoss:2.609\tAcc:6.000\n",
      "\tLoss:2.433\tAcc:8.000\n",
      "\tLoss:2.318\tAcc:8.000\n",
      "\tLoss:2.636\tAcc:10.000\n",
      "\tLoss:2.531\tAcc:10.000\n",
      "\tLoss:2.933\tAcc:8.000\n",
      "\tLoss:2.835\tAcc:12.000\n",
      "\tLoss:2.682\tAcc:10.000\n",
      "\tLoss:2.829\tAcc:14.000\n",
      "\tLoss:2.726\tAcc:8.000\n",
      "\tLoss:2.278\tAcc:14.000\n",
      "\tLoss:3.107\tAcc:12.000\n",
      "\tLoss:2.290\tAcc:14.000\n",
      "\tLoss:2.848\tAcc:6.000\n",
      "\tLoss:2.806\tAcc:8.000\n",
      "\tLoss:2.724\tAcc:10.000\n",
      "\tLoss:2.329\tAcc:6.000\n",
      "\tLoss:2.346\tAcc:8.000\n",
      "epoch6\n",
      "\tLoss:2.846\tAcc:8.000\n",
      "\tLoss:2.324\tAcc:8.000\n",
      "\tLoss:2.312\tAcc:18.000\n",
      "\tLoss:3.222\tAcc:12.000\n",
      "\tLoss:2.424\tAcc:18.000\n",
      "\tLoss:2.580\tAcc:6.000\n",
      "\tLoss:2.524\tAcc:14.000\n",
      "\tLoss:2.633\tAcc:4.000\n",
      "\tLoss:3.096\tAcc:10.000\n",
      "\tLoss:2.640\tAcc:10.000\n",
      "\tLoss:2.419\tAcc:20.000\n",
      "\tLoss:2.820\tAcc:12.000\n",
      "\tLoss:2.406\tAcc:8.000\n",
      "\tLoss:2.574\tAcc:8.000\n",
      "\tLoss:2.402\tAcc:10.000\n",
      "\tLoss:2.508\tAcc:12.000\n",
      "\tLoss:2.432\tAcc:14.000\n",
      "\tLoss:2.966\tAcc:8.000\n",
      "\tLoss:2.658\tAcc:12.000\n",
      "\tLoss:2.437\tAcc:12.000\n",
      "\tLoss:2.260\tAcc:22.000\n",
      "\tLoss:2.342\tAcc:10.000\n",
      "\tLoss:2.418\tAcc:8.000\n",
      "\tLoss:2.531\tAcc:12.000\n",
      "\tLoss:2.364\tAcc:18.000\n",
      "\tLoss:2.460\tAcc:4.000\n",
      "\tLoss:2.337\tAcc:10.000\n",
      "\tLoss:2.506\tAcc:16.000\n",
      "epoch7\n",
      "\tLoss:2.621\tAcc:8.000\n",
      "\tLoss:2.444\tAcc:14.000\n",
      "\tLoss:2.499\tAcc:8.000\n",
      "\tLoss:2.303\tAcc:12.000\n",
      "\tLoss:2.850\tAcc:14.000\n",
      "\tLoss:2.296\tAcc:10.000\n",
      "\tLoss:2.454\tAcc:12.000\n",
      "\tLoss:2.343\tAcc:8.000\n",
      "\tLoss:2.731\tAcc:18.000\n",
      "\tLoss:2.334\tAcc:8.000\n",
      "\tLoss:2.492\tAcc:4.000\n",
      "\tLoss:2.443\tAcc:8.000\n",
      "\tLoss:2.522\tAcc:6.000\n",
      "\tLoss:2.408\tAcc:6.000\n",
      "\tLoss:2.364\tAcc:12.000\n",
      "\tLoss:2.260\tAcc:12.000\n",
      "\tLoss:2.242\tAcc:10.000\n",
      "\tLoss:2.692\tAcc:14.000\n",
      "\tLoss:2.949\tAcc:6.000\n",
      "\tLoss:2.373\tAcc:12.000\n",
      "\tLoss:2.804\tAcc:10.000\n",
      "\tLoss:2.320\tAcc:10.000\n",
      "\tLoss:2.491\tAcc:6.000\n",
      "\tLoss:2.383\tAcc:8.000\n",
      "\tLoss:2.717\tAcc:10.000\n",
      "\tLoss:2.229\tAcc:8.000\n",
      "\tLoss:2.474\tAcc:12.000\n",
      "\tLoss:2.479\tAcc:12.000\n",
      "epoch8\n",
      "\tLoss:2.390\tAcc:14.000\n",
      "\tLoss:2.268\tAcc:10.000\n",
      "\tLoss:2.928\tAcc:16.000\n",
      "\tLoss:2.269\tAcc:10.000\n",
      "\tLoss:2.764\tAcc:14.000\n",
      "\tLoss:2.265\tAcc:10.000\n",
      "\tLoss:2.290\tAcc:10.000\n",
      "\tLoss:2.332\tAcc:6.000\n",
      "\tLoss:4.202\tAcc:8.000\n",
      "\tLoss:2.377\tAcc:10.000\n",
      "\tLoss:2.420\tAcc:10.000\n",
      "\tLoss:2.656\tAcc:12.000\n",
      "\tLoss:2.311\tAcc:4.000\n",
      "\tLoss:2.393\tAcc:4.000\n",
      "\tLoss:2.414\tAcc:10.000\n",
      "\tLoss:2.841\tAcc:16.000\n",
      "\tLoss:2.621\tAcc:10.000\n",
      "\tLoss:2.291\tAcc:16.000\n",
      "\tLoss:2.292\tAcc:8.000\n",
      "\tLoss:2.701\tAcc:8.000\n",
      "\tLoss:2.390\tAcc:10.000\n",
      "\tLoss:2.682\tAcc:8.000\n",
      "\tLoss:2.531\tAcc:10.000\n",
      "\tLoss:2.598\tAcc:20.000\n",
      "\tLoss:3.583\tAcc:14.000\n",
      "\tLoss:2.304\tAcc:10.000\n",
      "\tLoss:2.435\tAcc:10.000\n",
      "\tLoss:2.554\tAcc:14.000\n",
      "epoch9\n",
      "\tLoss:2.402\tAcc:12.000\n",
      "\tLoss:2.664\tAcc:12.000\n",
      "\tLoss:2.578\tAcc:14.000\n",
      "\tLoss:2.243\tAcc:20.000\n",
      "\tLoss:2.500\tAcc:12.000\n",
      "\tLoss:2.432\tAcc:16.000\n",
      "\tLoss:2.561\tAcc:4.000\n",
      "\tLoss:2.359\tAcc:10.000\n",
      "\tLoss:2.880\tAcc:2.000\n",
      "\tLoss:2.330\tAcc:6.000\n",
      "\tLoss:2.334\tAcc:8.000\n",
      "\tLoss:2.299\tAcc:10.000\n",
      "\tLoss:2.275\tAcc:18.000\n",
      "\tLoss:2.311\tAcc:8.000\n",
      "\tLoss:2.343\tAcc:10.000\n",
      "\tLoss:2.379\tAcc:8.000\n",
      "\tLoss:2.285\tAcc:12.000\n",
      "\tLoss:3.725\tAcc:10.000\n",
      "\tLoss:2.321\tAcc:24.000\n",
      "\tLoss:2.400\tAcc:6.000\n",
      "\tLoss:2.332\tAcc:18.000\n",
      "\tLoss:2.272\tAcc:20.000\n",
      "\tLoss:2.342\tAcc:6.000\n",
      "\tLoss:2.228\tAcc:6.000\n",
      "\tLoss:2.319\tAcc:8.000\n",
      "\tLoss:2.659\tAcc:8.000\n",
      "\tLoss:2.371\tAcc:14.000\n",
      "\tLoss:2.446\tAcc:8.000\n",
      "epoch10\n",
      "\tLoss:2.666\tAcc:6.000\n",
      "\tLoss:2.691\tAcc:12.000\n",
      "\tLoss:2.616\tAcc:14.000\n",
      "\tLoss:2.647\tAcc:16.000\n",
      "\tLoss:2.265\tAcc:12.000\n",
      "\tLoss:3.082\tAcc:8.000\n",
      "\tLoss:2.275\tAcc:12.000\n",
      "\tLoss:2.355\tAcc:10.000\n",
      "\tLoss:2.614\tAcc:12.000\n",
      "\tLoss:2.307\tAcc:10.000\n",
      "\tLoss:2.391\tAcc:2.000\n",
      "\tLoss:2.350\tAcc:4.000\n",
      "\tLoss:2.428\tAcc:10.000\n",
      "\tLoss:2.493\tAcc:10.000\n",
      "\tLoss:2.346\tAcc:4.000\n",
      "\tLoss:2.512\tAcc:10.000\n",
      "\tLoss:2.420\tAcc:14.000\n",
      "\tLoss:2.582\tAcc:8.000\n",
      "\tLoss:2.412\tAcc:10.000\n",
      "\tLoss:2.313\tAcc:12.000\n",
      "\tLoss:4.083\tAcc:8.000\n",
      "\tLoss:2.693\tAcc:6.000\n",
      "\tLoss:3.579\tAcc:14.000\n",
      "\tLoss:2.568\tAcc:12.000\n",
      "\tLoss:2.355\tAcc:6.000\n",
      "\tLoss:2.307\tAcc:6.000\n",
      "\tLoss:2.309\tAcc:10.000\n",
      "\tLoss:2.318\tAcc:8.000\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable#\n",
    "#torch.manual_seed(1)\n",
    "\n",
    "\n",
    "model = CNN_spoken_digit_dataset()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "train_loss = []\n",
    "train_acc  = []\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "        \n",
    "    print('epoch'+str(epoch+1))\n",
    "    for i,(data, label) in enumerate(train_dataset):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = torch.nn.functional.nll_loss(output, label)\n",
    "        loss.backward()\n",
    "        train_loss.append(loss.item())\n",
    "        optimizer.step()\n",
    "        \n",
    "        total = label.size(0)\n",
    "        preds = output.data.max(1)[1]\n",
    "        correct = (preds==label).sum().item()\n",
    "        accuracy = correct/BATCH_SIZE*100\n",
    "        train_acc.append( accuracy )\n",
    "        \n",
    "        \n",
    "        print('\\tLoss:{:.3f}\\tAcc:{:.3f}'.format(loss.item(),accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  11.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, label in test_dataset:\n",
    "        output = model(data)\n",
    "        preds  = torch.max(output.data, 1)[1]\n",
    "        total   += len(label)\n",
    "        correct += (preds==label).sum().item()\n",
    "    print('Test Accuracy: ', 100.*correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
